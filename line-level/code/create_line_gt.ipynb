{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mode = 'original_' # 'original_', ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(f'../dataset/py150/line_completion/', f\"{dataset_mode}test.json\")\n",
    "with open(datafile) as f:\n",
    "    datas = f.readlines()\n",
    "length = len(datas)\n",
    "inputs = []\n",
    "inputs_id = []\n",
    "ids = []\n",
    "# gts = []\n",
    "for data in datas:\n",
    "    data = json.loads(data.strip())\n",
    "    ids.append(data[\"id\"])\n",
    "    inputs.append(data['input'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../CodeCompletion-token/dataset/javaCorpus/token_completion/test.txt') as f:\n",
    "with open(f'../../CodeCompletion-token/dataset/py150/token_completion/{dataset_mode}test.txt') as f:\n",
    "    datas = f.readlines()\n",
    "with open(f'../../CodeCompletion-token/dataset/py150/token_completion/{dataset_mode}type_test.txt') as f:\n",
    "    type_datas = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../CodeCompletion-token/dataset/javaCorpus/token_completion/dev.txt') as f:\n",
    "with open(f'../../CodeCompletion-token/dataset/py150/token_completion/{dataset_mode}dev.txt') as f:\n",
    "    datas_dev = f.readlines()\n",
    "with open(f'../../CodeCompletion-token/dataset/py150/token_completion/{dataset_mode}type_dev.txt') as f:\n",
    "    type_datas_dev = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_line_level_dict(ids, inputs, gts):\n",
    "    line_dataset = []\n",
    "    for i in range(len(inputs)):\n",
    "        line = {}\n",
    "        line[\"id\"] = ids[i]\n",
    "        line['input'] = inputs[i]\n",
    "        line['gt'] = gts[i].strip()\n",
    "        line_dataset.append(line)\n",
    "    return line_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_line_dataset(line_dataset, filename='gts.json'):\n",
    "    with open(filename,\"a\") as f:\n",
    "        for i in range(len(line_dataset)):\n",
    "            json.dump(line_dataset[i], f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeat_types(tokens_list, types_lists, tokenizer, has_start_token=True):\n",
    "    new_gts_type = []\n",
    "    for j in tqdm(range(len(tokens_list))):\n",
    "        gt_types = types_lists[j].split(' ')\n",
    "        gt_tokens = tokenizer.tokenize(tokens_list[j].strip())\n",
    "        new_gt_types = []\n",
    "        i = 0\n",
    "        next_type = False\n",
    "        is_new_data = (i == 0 and not has_start_token)\n",
    "        for tok in gt_tokens:\n",
    "            if not is_new_data:\n",
    "                if tok[0] == 'Ġ' or tok.startswith('<NUM_LIT') or next_type:\n",
    "                    i+=1\n",
    "                    next_type = False\n",
    "                    if tok in ['<EOL>','<INDENT>','<DEDENT>'] or tok.startswith('<NUM_LIT'):\n",
    "                        next_type = True\n",
    "                elif tok in ['<EOL>','<INDENT>','<DEDENT>'] or tok.startswith('<NUM_LIT'):\n",
    "                    next_type = True\n",
    "                    i+=1\n",
    "            is_new_data = False\n",
    "            new_gt_types.append(gt_types[i])\n",
    "        if new_gt_types[-2].strip() == '</s>':\n",
    "            new_gt_types = new_gt_types[:-1]\n",
    "        # if gt_types[i].strip() != '</s>' or tok.strip() != '</s>':\n",
    "            # print(gt_types[i], tok)\n",
    "            # print(j, 'Error')\n",
    "            # break\n",
    "        new_gts_type.append(' '.join(new_gt_types))\n",
    "    return new_gts_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create line-level gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = []\n",
    "data_types = []\n",
    "data_gts = []\n",
    "data_type_gts = []\n",
    "for g,k in enumerate(ids):\n",
    "    new_data = datas[k].split()\n",
    "    gt_data = inputs[g].split()\n",
    "    type_gt_data = type_datas[k].split()\n",
    "    inp = []\n",
    "    typ = []\n",
    "    gt = []\n",
    "    typ_gt = []\n",
    "    j = 0\n",
    "    for i in range(len(gt_data)):\n",
    "        while new_data[j] in ['<INDENT>','<DEDENT>']:\n",
    "            inp.append(new_data[j])\n",
    "            typ.append(type_gt_data[j])\n",
    "            j+=1\n",
    "        if new_data[j] != gt_data[i]:\n",
    "            print(f'error -> data: {new_data[j]} \\t gt: {gt_data[i]}')\n",
    "            break\n",
    "        inp.append(new_data[j])\n",
    "        typ.append(type_gt_data[j])\n",
    "        j+=1\n",
    "    for i in range(j, len(new_data)):\n",
    "        if new_data[i] == '<EOL>':\n",
    "            break\n",
    "        gt.append(new_data[i])\n",
    "        typ_gt.append(type_gt_data[i])\n",
    "    data_inputs.append(\" \".join(inp))\n",
    "    data_types.append(\" \".join(typ))\n",
    "    data_gts.append(\" \".join(gt))\n",
    "    data_type_gts.append(\" \".join(typ_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gts_list = [d.split() for d in data_gts]\n",
    "data_type_gts_list = [d.split() for d in data_type_gts]\n",
    "data_inputs_list = [d.split() for d in data_inputs]\n",
    "data_types_list = [d.split() for d in data_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dataset = arrange_line_level_dict(ids, data_inputs, data_gts)\n",
    "type_line_dataset = arrange_line_level_dict(ids, data_types, data_type_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_line_dataset(line_dataset, 'test.json')\n",
    "# save_line_dataset(type_line_dataset, 'type_test.json')\n",
    "# save_line_dataset(concat_type_code_line_dataset, 'concat_type_code_test.json')\n",
    "# save_line_dataset(concat_code_type_line_dataset, 'concat_code_type_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "def get_special_tokens(path):\n",
    "    lits = json.load(open(path))\n",
    "    tokens = [\"<STR_LIT>\", \"<NUM_LIT>\", \"<CHAR_LIT>\"]\n",
    "    for lit in lits[\"str\"]:\n",
    "        tokens.append(f\"<STR_LIT:{lit}>\")\n",
    "    for lit in lits[\"num\"]:\n",
    "        tokens.append(f\"<NUM_LIT:{lit}>\")\n",
    "    for lit in lits[\"char\"]:\n",
    "        tokens.append(f\"<CHAR_LIT:{lit}>\")\n",
    "    return tokens\n",
    "special_tokens = get_special_tokens('../dataset/py150/literals.json')\n",
    "special_tokens.extend(['<NAME>', '<KEYWORD>', '<NUMBER>', '<STRING>', '<NEWLINE>', '<INDENT>', '<DEDENT>', '<LPAR>', '<RPAR>', '<LSQB>', '<RSQB>', '<COLON>', '<COMMA>', '<SEMI>', '<PLUS>', '<MINUS>', '<STAR>', '<SLASH>', '<VBAR>', '<AMPER>', '<LESS>', '<GREATER>', '<EQUAL>', '<DOT>', '<PERCENT>', '<LBRACE>', '<RBRACE>', '<EQEQUAL>', '<NOTEQUAL>', '<LESSEQUAL>', '<GREATEREQUAL>', '<TILDE>', '<CIRCUMFLEX>', '<LEFTSHIFT>', '<RIGHTSHIFT>', '<DOUBLESTAR>', '<PLUSEQUAL>', '<MINEQUAL>', '<STAREQUAL>', '<SLASHEQUAL>', '<PERCENTEQUAL>', '<AMPEREQUAL>', '<VBAREQUAL>', '<CIRCUMFLEXEQUAL>', '<LEFTSHIFTEQUAL>', '<RIGHTSHIFTEQUAL>', '<DOUBLESTAREQUAL>', '<DOUBLESLASH>', '<DOUBLESLASHEQUAL>', '<AT>', '<ATEQUAL>', '<RARROW>', '<ELLIPSIS>', '<ERRORTOKEN>'])\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/CodeGPT-small-py', do_lower_case=False, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', additional_special_tokens=special_tokens)\n",
    "# -adaptedGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 866.68it/s]\n",
      "100%|██████████| 10000/10000 [09:59<00:00, 16.68it/s]\n"
     ]
    }
   ],
   "source": [
    "repeat_gts_type_test = create_repeat_types(data_gts, data_type_gts, tokenizer, has_start_token=False)\n",
    "repeat_type_test = create_repeat_types(data_inputs, data_types, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 852.51it/s]\n",
      "100%|██████████| 10000/10000 [10:00<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# repeat_adapt_gts_type_test = create_repeat_types(data_gts, data_type_gts, tokenizer, has_start_token=False)\n",
    "# repeat_adapt_type_test = create_repeat_types(data_inputs, data_types, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_type_line_dataset = arrange_line_level_dict(ids, repeat_type_test, repeat_gts_type_test)\n",
    "# repeat_adapt_type_line_dataset = arrange_line_level_dict(ids, repeat_adapt_type_test, repeat_adapt_gts_type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_line_dataset(repeat_type_line_dataset, 'original_type_repeat_test.json')\n",
    "# save_line_dataset(repeat_adapt_type_line_dataset, 'type_adapt_repeat_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Line-level for Dev set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis statistic of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_tokens(data_list):\n",
    "    avg = 0\n",
    "    min_len = len(data_list[0])\n",
    "    max_len = len(data_list[0])\n",
    "    min_idx = 0\n",
    "    max_idx = 0\n",
    "    for i in range(len(data_list)):\n",
    "        avg += len(data_list[i])\n",
    "        if len(data_list[i]) < min_len:\n",
    "            min_len = len(data_list[i])  \n",
    "            min_idx = i\n",
    "        if len(data_list[i]) > max_len:\n",
    "            max_len = len(data_list[i])  \n",
    "            max_idx = i\n",
    "    avg /= len(data_list)\n",
    "    print('### Line-level Test datset ###')\n",
    "    print('# input data')\n",
    "    print(f'avg tokens: {avg}')\n",
    "    print(f'min lens: {min_len}')\n",
    "    print(f'max lens: {max_len}')\n",
    "    return min_idx, max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 502.2463\n",
      "min lens: 11\n",
      "max lens: 21333\n",
      "\n",
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 6.7219\n",
      "min lens: 2\n",
      "max lens: 46\n"
     ]
    }
   ],
   "source": [
    "min_idx_input_test, max_idx_input_test = analysis_tokens(data_inputs_list)\n",
    "print()\n",
    "min_idx_gts_test, max_idx_gts_test = analysis_tokens(data_gts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_dev_list = [d.split() for d in datas_dev]\n",
    "datas_test_list = [d.split() for d in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 1066.773\n",
      "min lens: 3\n",
      "max lens: 38819\n",
      "\n",
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 897.6456\n",
      "min lens: 3\n",
      "max lens: 39799\n"
     ]
    }
   ],
   "source": [
    "min_idx_dev, max_idx_dev = analysis_tokens(datas_dev_list)\n",
    "print()\n",
    "min_idx_test, max_idx_test = analysis_tokens(datas_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed = 42\n",
    "ids_shuffle = [i for i in range(5000)]\n",
    "random.Random(random_seed).shuffle(ids_shuffle)\n",
    "data_inputs_dev = []\n",
    "data_gts_dev = []\n",
    "type_inputs_dev = []\n",
    "type_gts_dev = []\n",
    "ids_dev = []\n",
    "for i in ids_shuffle:\n",
    "    data_dev_list = datas_dev[i].split()\n",
    "    type_dev_list = type_datas_dev[i].split()\n",
    "    if len(data_dev_list) <= 15:\n",
    "        continue\n",
    "    tokens_len = random.randint(10, len(data_dev_list)-3)\n",
    "    if data_dev_list[tokens_len] == '<EOL>':\n",
    "        tokens_len += 1\n",
    "    for j in range(tokens_len, len(data_dev_list)):\n",
    "        if data_dev_list[j] == \"<EOL>\":\n",
    "            break\n",
    "    if j-tokens_len <= 1 or j-tokens_len > 100:\n",
    "        continue\n",
    "    data_inputs_dev.append(\" \".join(data_dev_list[:tokens_len]))\n",
    "    type_inputs_dev.append(\" \".join(type_dev_list[:tokens_len]))\n",
    "    data_gts_dev.append(\" \".join(data_dev_list[tokens_len:j]))\n",
    "    type_gts_dev.append(\" \".join(type_dev_list[tokens_len:j]))\n",
    "    ids_dev.append(i)\n",
    "    if len(data_inputs_dev) == 2500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_line_dataset = arrange_line_level_dict(ids_dev, data_inputs_dev, data_gts_dev)\n",
    "dev_type_line_dataset = arrange_line_level_dict(ids_dev, type_inputs_dev, type_gts_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_line_dataset(dev_line_dataset, 'dev.json')\n",
    "# save_line_dataset(dev_type_line_dataset, 'type_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:02<00:00, 910.81it/s]\n",
      "100%|██████████| 2500/2500 [02:47<00:00, 14.93it/s]\n"
     ]
    }
   ],
   "source": [
    "repeat_gts_type_dev = create_repeat_types(data_gts_dev, type_gts_dev, tokenizer, has_start_token=False)\n",
    "repeat_type_dev = create_repeat_types(data_inputs_dev, type_inputs_dev, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:02<00:00, 865.22it/s]\n",
      "100%|██████████| 2500/2500 [02:46<00:00, 15.05it/s]\n"
     ]
    }
   ],
   "source": [
    "repeat_adapt_gts_type_dev = create_repeat_types(data_gts_dev, type_gts_dev, tokenizer, has_start_token=False)\n",
    "repeat_adapt_type_dev = create_repeat_types(data_inputs_dev, type_inputs_dev, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_repeat_type_line_dataset = arrange_line_level_dict(ids_dev, repeat_type_dev, repeat_gts_type_dev)\n",
    "dev_repeat_adapt_type_line_dataset = arrange_line_level_dict(ids_dev, repeat_adapt_type_dev, repeat_adapt_gts_type_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_line_dataset(dev_repeat_type_line_dataset, 'type_repeat_dev.json')\n",
    "# save_line_dataset(dev_repeat_adapt_type_line_dataset, 'type_adapt_repeat_dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis len of dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gts_dev_list = [d.split() for d in data_gts_dev]\n",
    "data_inputs_dev_list = [d.split() for d in data_inputs_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 557.892\n",
      "min lens: 10\n",
      "max lens: 29961\n",
      "\n",
      "### Line-level Test datset ###\n",
      "# input data\n",
      "avg tokens: 6.206\n",
      "min lens: 2\n",
      "max lens: 51\n"
     ]
    }
   ],
   "source": [
    "min_idx_input_dev, max_idx_input_dev = analysis_tokens(data_inputs_dev_list)\n",
    "print()\n",
    "min_idx_gts_dev, max_idx_gts_dev = analysis_tokens(data_gts_dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> import json <EOL> import base64 <EOL> import pytest <EOL> import jwt . exceptions <EOL> from django . test import TestCase <EOL> from rest_framework_jwt import utils <EOL> from rest_framework_jwt . compat import get_user_model <EOL> from rest_framework_jwt . settings import api_settings , DEFAULTS <EOL> User = get_user_model ( ) <EOL> def base64url_decode ( input ) : <EOL> <INDENT> rem = len ( input ) % <NUM_LIT:4> <EOL> if rem > <NUM_LIT:0> : <EOL> <INDENT> input += b'<STR_LIT:=>' * ( <NUM_LIT:4> - rem ) <EOL> <DEDENT> return base64 . urlsafe_b64decode ( input ) <EOL> <DEDENT> class UtilsTests ( TestCase ) : <EOL> <INDENT> def setUp ( self\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inputs_dev[min_idx_gts_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "') :'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gts_dev[min_idx_gts_dev]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca72d922c08c5f96bebc5c54e88916ae61bd21443384f0f95e31e832fe61134d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('codexglue': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
