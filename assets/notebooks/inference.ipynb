{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "lit_file = '../../token-level/dataset/py150/literals.json'\n",
    "model_dir = 'Wannita/PyCoder' # or your trained PyCoder model dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_tokens(path):\n",
    "    lits = json.load(open(path))\n",
    "    tokens = [\"<STR_LIT>\", \"<NUM_LIT>\", \"<CHAR_LIT>\"]\n",
    "    for lit in lits[\"str\"]:\n",
    "        tokens.append(f\"<STR_LIT:{lit}>\")\n",
    "    for lit in lits[\"num\"]:\n",
    "        tokens.append(f\"<NUM_LIT:{lit}>\")\n",
    "    for lit in lits[\"char\"]:\n",
    "        tokens.append(f\"<CHAR_LIT:{lit}>\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50288, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get special tokens\n",
    "special_tokens = get_special_tokens(lit_file)\n",
    "token_types = ['<NAME>', '<KEYWORD>', '<NUMBER>', '<STRING>', '<NEWLINE>', '<INDENT>', '<DEDENT>', '<LPAR>', '<RPAR>', '<LSQB>', '<RSQB>', '<COLON>', '<COMMA>', '<SEMI>', '<PLUS>', '<MINUS>', '<STAR>', '<SLASH>', '<VBAR>', '<AMPER>', '<LESS>', '<GREATER>', '<EQUAL>', '<DOT>', '<PERCENT>', '<LBRACE>', '<RBRACE>', '<EQEQUAL>', '<NOTEQUAL>', '<LESSEQUAL>', '<GREATEREQUAL>', '<TILDE>', '<CIRCUMFLEX>', '<LEFTSHIFT>', '<RIGHTSHIFT>', '<DOUBLESTAR>', '<PLUSEQUAL>', '<MINEQUAL>', '<STAREQUAL>', '<SLASHEQUAL>', '<PERCENTEQUAL>', '<AMPEREQUAL>', '<VBAREQUAL>', '<CIRCUMFLEXEQUAL>', '<LEFTSHIFTEQUAL>', '<RIGHTSHIFTEQUAL>', '<DOUBLESTAREQUAL>', '<DOUBLESLASH>', '<DOUBLESLASHEQUAL>', '<AT>', '<ATEQUAL>', '<RARROW>', '<ELLIPSIS>', '<ERRORTOKEN>']\n",
    "special_tokens.extend(token_types)\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir, do_lower_case=False, sep_token='<EOL>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', additional_special_tokens=special_tokens)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeIds(idxs):\n",
    "    codes = \"\"\n",
    "    for idx in idxs:\n",
    "        to_add = tokenizer.convert_ids_to_tokens(idx)\n",
    "        if tokenizer.convert_ids_to_tokens(idx)[0] == '\\u0120':\n",
    "            if not codes.endswith(\" \"):\n",
    "                codes += \" \" + to_add[1:]\n",
    "            else:\n",
    "                codes += to_add[1:]\n",
    "        elif (\n",
    "            idx in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id] or\n",
    "            tokenizer.convert_ids_to_tokens(idx).startswith(\"<NUM_LIT\") or to_add in token_types\n",
    "        ):\n",
    "            if not codes.endswith(\" \"):\n",
    "                codes += \" \" + to_add + \" \"\n",
    "            else:\n",
    "                codes += to_add + \" \"\n",
    "        else:\n",
    "            codes += to_add\n",
    "    return codes.strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "token_test = open('../../token-level/dataset/py150/token_completion/test.txt').readlines() \n",
    "line_datas = open('../../line-level/dataset/py150/line_completion/test.json').readlines()\n",
    "line_inputs = []\n",
    "line_gts = []\n",
    "for data in line_datas:\n",
    "    data = json.loads(data.strip())\n",
    "    line_inputs.append(data[\"input\"])\n",
    "    line_gts.append([data[\"gt\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> from django . utils . translation import ugettext_lazy as _ <EOL> from horizon import tabs <EOL> class NetworkProfileTab ( tabs . Tab ) : <EOL> <INDENT> name = _ ( \"<STR_LIT>\" ) <EOL> slug = \"<STR_LIT>\" <EOL> template_name = \\'<STR_LIT>\\' <EOL> def get_context_data ( self , request ) : <EOL> <INDENT> return None <EOL> <DEDENT> <DEDENT> class PolicyProfileTab ( tabs . Tab ) : <EOL> <INDENT> name = _ ( \"<STR_LIT>\" ) <EOL> slug = \"<STR_LIT>\" <EOL> template_name = \\'<STR_LIT>\\' <EOL> preload = False <EOL> <DEDENT> class IndexTabs ( tabs . TabGroup ) : <EOL> <INDENT> slug = \"<STR_LIT>\" <EOL> tabs = ( NetworkProfileTab , PolicyProfileTab ) <EOL> <DEDENT> <EOL> </s>\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> import threading <EOL> import IECore <EOL> import Gaffer <EOL> import GafferUI <EOL> import GafferImage <EOL> __all__ = [ ] <EOL> Gaffer . Metadata . registerNode ( <EOL> GafferImage . Display , <EOL> \"<STR_LIT:description>\" , <EOL> \"\"\"<STR_LIT>\"\"\" , <EOL> plugs = { <EOL> \"<STR_LIT:port>\" : [ <EOL> \"<STR_LIT:description>\" , <EOL> \"\"\"<STR_LIT>\"\"\" , <EOL> ] , <EOL> } <EOL> ) <EOL> __plugsPendingUpdate = [ ] <EOL> __plugsPendingUpdateLock = threading . Lock ( ) <EOL> def __scheduleUpdate ( plug , force = False ) : <EOL> <INDENT> if not force : <EOL> <INDENT> global __plugsPendingUpdate <EOL> global __plugsPendingUpdateLock <EOL> with __plugsPendingUpdateLock : <EOL> <INDENT> for p in __plugsPendingUpdate : <EOL> <INDENT> if plug . isSame ( p ) : <EOL> <INDENT> return <EOL> <DEDENT> <DEDENT> __plugsPendingUpdate . append ( plug ) <EOL> <DEDENT> <DEDENT> GafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) ) <EOL> <DEDENT> def __update ( plug ) : <EOL> <INDENT> node = plug . node ( ) <EOL> if node : <EOL> <INDENT> updateCountPlug = node [ \"<STR_LIT>\" ] <EOL> updateCountPlug . setValue ( updateCountPlug . getValue ( )'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_inputs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wannita/.conda/envs/novel_pt/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 264, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 265, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> import threading <EOL> import IECore <EOL> import Gaffer <EOL> import GafferUI <EOL> import GafferImage <EOL> __all__ = [ ] <EOL> Gaffer . Metadata . registerNode ( <EOL> GafferImage . Display , <EOL> \"<STR_LIT:description>\" , <EOL> \"\"\"<STR_LIT>\"\"\" , <EOL> plugs = { <EOL> \"<STR_LIT:port>\" : [ <EOL> \"<STR_LIT:description>\" , <EOL> \"\"\"<STR_LIT>\"\"\" , <EOL> ] , <EOL> } <EOL> ) <EOL> __plugsPendingUpdate = [ ] <EOL> __plugsPendingUpdateLock = threading . Lock ( ) <EOL> def __scheduleUpdate ( plug , force = False ) : <EOL> <INDENT> if not force : <EOL> <INDENT> global __plugsPendingUpdate <EOL> global __plugsPendingUpdateLock <EOL> with __plugsPendingUpdateLock : <EOL> <INDENT> for p in __plugsPendingUpdate : <EOL> <INDENT> if plug . isSame ( p ) : <EOL> <INDENT> return <EOL> <DEDENT> <DEDENT> __plugsPendingUpdate . append ( plug ) <EOL> <DEDENT> <DEDENT> GafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) ) <EOL> <DEDENT> def __update ( plug ) : <EOL> <INDENT> node = plug . node ( ) <EOL> if node : <EOL> <INDENT> updateCountPlug = node [ \"<STR_LIT>\" ] <EOL> updateCountPlug . setValue ( updateCountPlug . getValue ( ) ) <EOL>\n"
     ]
    }
   ],
   "source": [
    "# predict one token until reach <EOL>\n",
    "text = line_inputs[0]\n",
    "block_size = 924\n",
    "while True:\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids[:, -block_size:].to(device)\n",
    "    generated_ids = model.generate(input_ids) #, max_length=block_size)\n",
    "    text = DecodeIds(generated_ids[0].tolist())\n",
    "    if generated_ids[0][-1] == tokenizer.sep_token_id:\n",
    "        break\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 264, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 265, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ") <EOL>\n"
     ]
    }
   ],
   "source": [
    "# predict one token until reach <EOL> or gen_len \n",
    "# show only the predicted tokens\n",
    "text = line_inputs[0]\n",
    "block_size = 924\n",
    "gen_len = 100\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids[:, -block_size:].to(device)\n",
    "for i in range(gen_len):\n",
    "    input_ids = model.generate(input_ids) \n",
    "    if input_ids[0][-1] == tokenizer.sep_token_id:\n",
    "        break\n",
    "print(DecodeIds(input_ids[:, -i-1:][0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(code):\n",
    "    code = code.replace(\"<NUM_LIT>\", \"0\").replace(\"<STR_LIT>\", \"\").replace(\"<CHAR_LIT>\", \"\")\n",
    "    pattern = re.compile(r\"<(STR|NUM|CHAR)_LIT:(.*?)>\", re.S)\n",
    "    lits = re.findall(pattern, code)\n",
    "    for lit in lits:\n",
    "        code = code.replace(f\"<{lit[0]}_LIT:{lit[1]}>\", lit[1])\n",
    "    return code\n",
    "def clean_to_code(code_str, post_literal=False):\n",
    "    code = \"\"\n",
    "    if post_literal:\n",
    "        code_str = post_process(code_str)\n",
    "    code_str = code_str.replace('<s>', '')\n",
    "    code_str = code_str.replace('</s>', '')\n",
    "    code_list = code_str.split()\n",
    "    indent = 0\n",
    "    newline = False\n",
    "    for tok in code_list:\n",
    "        if '<NUM_LIT:' in tok:\n",
    "            tok = tok[len('<NUM_LIT:'):-1]\n",
    "        elif tok == '<NUM_LIT>':\n",
    "            tok = '0'\n",
    "        if tok ==  '<INDENT>':\n",
    "            indent += 1\n",
    "        elif tok == '<DEDENT>':\n",
    "            indent -= 1\n",
    "        elif tok == '<EOL>':\n",
    "            newline = True\n",
    "        else:\n",
    "            if newline:\n",
    "                code += '\\n'\n",
    "                newline = False\n",
    "                if indent > 0:\n",
    "                    code += '\\t' * indent\n",
    "                code += tok\n",
    "            else:\n",
    "                code += \" \" + tok\n",
    "    return code.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import threading\n",
      "import IECore\n",
      "import Gaffer\n",
      "import GafferUI\n",
      "import GafferImage\n",
      "__all__ = [ ]\n",
      "Gaffer . Metadata . registerNode (\n",
      "GafferImage . Display ,\n",
      "\"<STR_LIT:description>\" ,\n",
      "\"\"\"<STR_LIT>\"\"\" ,\n",
      "plugs = {\n",
      "\"<STR_LIT:port>\" : [\n",
      "\"<STR_LIT:description>\" ,\n",
      "\"\"\"<STR_LIT>\"\"\" ,\n",
      "] ,\n",
      "}\n",
      ")\n",
      "__plugsPendingUpdate = [ ]\n",
      "__plugsPendingUpdateLock = threading . Lock ( )\n",
      "def __scheduleUpdate ( plug , force = False ) :\n",
      "\tif not force :\n",
      "\t\tglobal __plugsPendingUpdate\n",
      "\t\tglobal __plugsPendingUpdateLock\n",
      "\t\twith __plugsPendingUpdateLock :\n",
      "\t\t\tfor p in __plugsPendingUpdate :\n",
      "\t\t\t\tif plug . isSame ( p ) :\n",
      "\t\t\t\t\treturn\n",
      "\t\t\t__plugsPendingUpdate . append ( plug )\n",
      "\tGafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) )\n",
      "def __update ( plug ) :\n",
      "\tnode = plug . node ( )\n",
      "\tif node :\n",
      "\t\tupdateCountPlug = node [ \"<STR_LIT>\" ]\n",
      "\t\tupdateCountPlug . setValue ( updateCountPlug . getValue ( )\n"
     ]
    }
   ],
   "source": [
    "print(clean_to_code(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the first step, in case the source code input still didn't pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import keyword\n",
    "from tokenize import tokenize, COMMENT, STRING, NEWLINE, ENCODING, ENDMARKER, NL, INDENT, NUMBER, DEDENT, ERRORTOKEN, NAME\n",
    "\n",
    "lits = json.load(open(lit_file))\n",
    "def process_string(token, special_chars={\" \": \"U+0020\", \",\": \"U+002C\"}):\n",
    "    str_quote_options = [\"'''\", '\"\"\"', \"'\", '\"']\n",
    "    start_quote = \"\"\n",
    "    end_quote = \"\"\n",
    "    qualifier_regex = r\"^[a-zA-Z]+\"\n",
    "    qualifier_match = re.search(qualifier_regex, token)\n",
    "    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)\n",
    "    qualifier = \"\" if not qualifier_match else qualifier_match[0]\n",
    "    # token string without qualifiers\n",
    "    token_string = re.sub(qualifier_regex, \"\", token)\n",
    "    # string literal without quotes\n",
    "    str_lit = token_string\n",
    "    for q in str_quote_options:\n",
    "        if token_string.startswith(q):\n",
    "            start_quote = q\n",
    "            str_lit = str_lit[len(q) :]\n",
    "            if token_string.endswith(q):\n",
    "                end_quote = q\n",
    "                str_lit = str_lit[: -len(q)]\n",
    "            break\n",
    "    # if start_quote in str_quote_options[:2]:\n",
    "    #     return \"\"\n",
    "    for sc in special_chars:\n",
    "        str_lit = str_lit.replace(sc, special_chars[sc])\n",
    "    return (\n",
    "        f\"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}\"\n",
    "        if str_lit in lits['str']\n",
    "        else f\"{qualifier}{start_quote}<STR_LIT>{end_quote}\"\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(input_code, close_tag=True):\n",
    "    #### extract exact token type from tokenzier library ####\n",
    "    ## set close_tag=False, if process the unfinish code ##\n",
    "    transform_dict = {\n",
    "        NL: \"<EOL>\",\n",
    "        NEWLINE: \"<EOL>\",\n",
    "        INDENT: \"<INDENT>\",\n",
    "        DEDENT: \"<DEDENT>\",\n",
    "    }\n",
    "    out_code = []\n",
    "    try:\n",
    "        token_gen = tokenize(input_code)\n",
    "        was_eol = False\n",
    "        for tok in token_gen:\n",
    "            toknum = tok.type\n",
    "            tokval = \" \".join(tok.string.split())\n",
    "            if toknum == ERRORTOKEN and tokval in [\" \",\"\"]:\n",
    "                continue\n",
    "            elif toknum in [NEWLINE, NL]:\n",
    "                if not was_eol:\n",
    "                    out_code.append(\"<EOL>\")\n",
    "                    was_eol = True\n",
    "            elif toknum in transform_dict:\n",
    "                out_code.append(transform_dict[toknum])\n",
    "                was_eol = False\n",
    "            elif toknum == NAME and keyword.iskeyword(tokval):\n",
    "                out_code.append(tokval)\n",
    "                was_eol = False\n",
    "            elif toknum == STRING:\n",
    "                add_token = process_string(tokval)\n",
    "                out_code.append(add_token)\n",
    "                was_eol = False\n",
    "            elif toknum == NUMBER: \n",
    "                if tokval in lits['num']:\n",
    "                    out_code.append(f\"<NUM_LIT:{tokval}>\")\n",
    "                else:\n",
    "                    out_code.append(f\"<NUM_LIT>\")\n",
    "                was_eol = False\n",
    "            elif toknum not in [COMMENT, ENCODING, ENDMARKER]:\n",
    "                out_code.append(tokval)\n",
    "                was_eol = False\n",
    "        if len(out_code) > 0 and out_code[0] == \"<EOL>\":\n",
    "            out_code = out_code[1:]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if close_tag:\n",
    "        if len(out_code) > 0 and out_code[0] == \"<EOL>\":\n",
    "            out_code.append(\"<EOL>\")\n",
    "        out_code = [\"<s>\"] + out_code + [\"</s>\"]\n",
    "    else:\n",
    "        out_code = [\"<s>\"] + out_code\n",
    "    out = \" \".join(out_code)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coderl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ae0ce8c525395a9092d1480fac9d2e116ed89746b76fe3c86575bb7caacb4d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
